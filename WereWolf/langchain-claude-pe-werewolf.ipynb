{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35db18b4-3f20-4090-9402-69ec7c684739",
   "metadata": {},
   "source": [
    "# Bedrock with LangChain using a Prompt that includes Context\n",
    "\n",
    "please refer the detail : https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/01_Generation/02_contextual_generation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe8c40c-20fe-4af0-864e-8e6c828de6b4",
   "metadata": {},
   "source": [
    "## install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca77056f-30e2-4871-8d34-9574db3ea726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "!pip install boto3>=1.28.59\n",
    "!pip install langchain\n",
    "!pip install langchainhub\n",
    "!pip install -U \"anthropic[bedrock]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65343449-e808-4e95-8085-db8cbf2cda73",
   "metadata": {},
   "source": [
    "## check bedrock availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8040cab-2932-4308-9ff7-4bc07ec3b904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def usage_demo():\n",
    "    \"\"\"\n",
    "    Shows how to list the available foundation models.\n",
    "    This demonstration gets the list of available foundation models and\n",
    "    prints their respective summaries.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    print(\"-\" * 88)\n",
    "    print(\"Welcome to the Amazon Bedrock demo.\")\n",
    "    print(\"-\" * 88)\n",
    "\n",
    "    bedrock_client = boto3.client(service_name=\"bedrock\", region_name=\"us-east-1\")\n",
    "\n",
    "    wrapper = BedrockWrapper(bedrock_client)\n",
    "\n",
    "    print(\"Listing the available foundation models.\")\n",
    "\n",
    "    try:\n",
    "        for model in wrapper.list_foundation_models():\n",
    "            print_model_details(model)\n",
    "    except ClientError:\n",
    "        logger.exception(\"Couldn't list foundation models.\")\n",
    "        raise\n",
    "\n",
    "    print(\"Getting the details of an individual foundation model.\")\n",
    "\n",
    "    model_id = \"amazon.titan-embed-text-v1\"\n",
    "\n",
    "    try:\n",
    "        print_model_details(wrapper.get_foundation_model(model_id))\n",
    "    except ClientError:\n",
    "        logger.exception(f\"Couldn't get foundation model {model_id}.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197fdf1e-f0cb-4fd1-a89c-6ba7296176da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock\n",
    "from utils.BedrockWrapper import BedrockWrapper\n",
    "from utils.GamePlayer import GamePlayer\n",
    "from utils.GameAssistant import GameAssistant\n",
    "from utils.GameMaster import GameMaster\n",
    "from utils.PeTemplates import *\n",
    "from utils import ParseJson, print_ww, Print, Info, Debug, Warn, Error\n",
    "from utils import print_model_details\n",
    "        \n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "usage_demo()\n",
    "#boto3_bedrock = bedrock.get_bedrock_client()\n",
    "\n",
    "#listModels = bedrock.list_foundation_models(byProvider='meta')\n",
    "\n",
    "# print_ww(\"hello\")\n",
    "# Info(\"hello\")\n",
    "\n",
    "# GA = GameAssistant(template_assistant_role, 1000)\n",
    "# print(GA.agent.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ca9a03-5f1f-46fc-915a-0fa6d0801335",
   "metadata": {},
   "source": [
    "## Test Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "129d5d72-0a98-480f-bca4-c26f0fcd716f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====claude3====\n",
      "\n",
      "----------------------------------------------------------------------------------------\n",
      "Invoking: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Prompt: 5+3等于多少？\n",
      "Completion: {'id': 'msg_0192MfrCHtdDeqVLWrHUZVke', 'type': 'message', 'role': 'assistant', 'content': [{'type': 'text', 'text': '5+3=8'}], 'model': 'claude-3-sonnet-28k-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 9}}\n",
      "\n",
      "====claude3 with stream response====\n",
      "\n",
      "{'type': 'message_start', 'message': {'id': 'msg_01XpPpRcWhGub3rebsywJtGZ', 'type': 'message', 'role': 'assistant', 'content': [], 'model': 'claude-3-sonnet-28k-20240229', 'stop_reason': None, 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 1}}}{'type': 'content_block_start', 'index': 0, 'content_block': {'type': 'text', 'text': ''}}{'type': 'content_block_delta', 'index': 0, 'delta': {'type': 'text_delta', 'text': '5'}}{'type': 'content_block_delta', 'index': 0, 'delta': {'type': 'text_delta', 'text': '+'}}{'type': 'content_block_delta', 'index': 0, 'delta': {'type': 'text_delta', 'text': '3'}}{'type': 'content_block_delta', 'index': 0, 'delta': {'type': 'text_delta', 'text': '='}}{'type': 'content_block_delta', 'index': 0, 'delta': {'type': 'text_delta', 'text': '8'}}{'type': 'content_block_stop', 'index': 0}{'type': 'message_delta', 'delta': {'stop_reason': 'end_turn', 'stop_sequence': None}, 'usage': {'output_tokens': 9}}{'type': 'message_stop', 'amazon-bedrock-invocationMetrics': {'inputTokenCount': 16, 'outputTokenCount': 8, 'invocationLatency': 594, 'firstByteLatency': 439}}"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils.BedrockRuntimeWrapper import BedrockRuntimeWrapper, invoke\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "client = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "wrapper = BedrockRuntimeWrapper(client)\n",
    "\n",
    "mode_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "text_generation_prompt = \"5+3等于多少？\"\n",
    "print(\"\\n====claude3====\\n\")\n",
    "invoke(wrapper, mode_id, text_generation_prompt)\n",
    "print(\"\\n====claude3 with stream response====\\n\")\n",
    "try:\n",
    "    async for completion in wrapper.invoke_claude3_with_response_stream(text_generation_prompt):\n",
    "        print(completion, end=\"\")\n",
    "\n",
    "except ClientError:\n",
    "    logger.exception(\"Couldn't invoke model %s\", model_id)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e6c8ee-6c1d-4477-9159-45c870987e44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain_community.llms import Bedrock\n",
    "\n",
    "bedrock = boto3.client('bedrock-runtime' , 'us-east-1')\n",
    "\n",
    "MODEL_KWARGS = {\n",
    "\"anthropic.claude-3-sonnet-20240229-v1:0\": {\n",
    "        \"temperature\": 0, \n",
    "        \"top_k\": 250, \n",
    "        \"top_p\": 1, \n",
    "        \"max_tokens_to_sample\": 2**10 \n",
    "}}\n",
    "\n",
    "model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "llm = Bedrock(model_id=model_id, model_kwargs=MODEL_KWARGS[model_id])\n",
    "llm('tell me a joke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936436a8-dede-4dce-b10d-a6ff30db8b00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from anthropic import AnthropicBedrock\n",
    "\n",
    "client = AnthropicBedrock(\n",
    "    # Authenticate by either providing the keys below or use the default AWS credential providers, such as\n",
    "    # using ~/.aws/credentials or the \"AWS_SECRET_ACCESS_KEY\" and \"AWS_ACCESS_KEY_ID\" environment variables.\n",
    "    #aws_access_key=\"<access key>\",\n",
    "    #aws_secret_key=\"<secret key>\",\n",
    "    # Temporary credentials can be used with aws_session_token.\n",
    "    # Read more at https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html.\n",
    "    #aws_session_token=\"<session_token>\",\n",
    "    # aws_region changes the aws region to which the request is made. By default, we read AWS_REGION,\n",
    "    # and if that's not present, we default to us-east-1. Note that we do not read ~/.aws/config for the region.\n",
    "    aws_region=\"us-east-1\",\n",
    ")\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    max_tokens=256,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, world\"}]\n",
    ")\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0576123a-855f-4322-9770-d8d1504b0898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\"\"\"\n",
    "Shows how to generate a message with Anthropic Claude (on demand).\n",
    "\"\"\"\n",
    "import boto3\n",
    "import json\n",
    "import logging\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def generate_message(bedrock_runtime, model_id, system_prompt, messages, max_tokens):\n",
    "\n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"system\": system_prompt,\n",
    "            \"messages\": messages\n",
    "        }  \n",
    "    )  \n",
    "\n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "   \n",
    "    return response_body\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Entrypoint for Anthropic Claude message example.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "\n",
    "        bedrock_runtime = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "        model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "        system_prompt = \"Please respond.\"\n",
    "        max_tokens = 1000\n",
    "\n",
    "        # Prompt with user turn only.\n",
    "        user_message =  {\"role\": \"user\", \"content\": \"你是谁？\"}\n",
    "        messages = [user_message]\n",
    "\n",
    "        response = generate_message (bedrock_runtime, model_id, system_prompt, messages, max_tokens)\n",
    "        print(\"User turn only.\")\n",
    "        print(json.dumps(response, indent=2))\n",
    "\n",
    "        # Prompt with both user turn and prefilled assistant response.\n",
    "        #Anthropic Claude continues by using the prefilled assistant text.\n",
    "        assistant_message =  {\"role\": \"assistant\", \"content\": \"<emoji>\"}\n",
    "        messages = [user_message, assistant_message]\n",
    "        response = generate_message(bedrock_runtime, model_id,system_prompt, messages, max_tokens)\n",
    "        print(\"User turn and prefilled assistant response.\")\n",
    "        print(json.dumps(response, indent=4))\n",
    "\n",
    "    except ClientError as err:\n",
    "        message=err.response[\"Error\"][\"Message\"]\n",
    "        logger.error(\"A client error occurred: %s\", message)\n",
    "        print(\"A client error occured: \" +\n",
    "            format(message))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c9faaf-0fcc-4389-bb51-1d93031b93bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.PeTemplates import *\n",
    "from langchain.agents import tool\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "    \n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "model = claude3_Sonnet\n",
    "# And a query intended to prompt a language model to populate the data structure.\n",
    "prompt_and_model = prompt | model\n",
    "output = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})\n",
    "parser.invoke(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898dab35-0dd0-49ee-bf88-b795a73cc607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.CustomTools import *\n",
    "from utils.PeTemplates import *\n",
    "from langchain.agents import tool\n",
    "\n",
    "# - 夜晚投票(狼人专属行动): WolfVote 参数: target=存活玩家\n",
    "# - 夜晚查验(预言家专属行动): ProphetCheck 参数: target=存活玩家\n",
    "# - 白天怀疑(所有玩家白天可选行动, 非投票): PlayerDoubt 参数: target=存活玩家 \n",
    "# - 白天投票: PlayerVote 参数: target=存活玩家 \n",
    "# - 白天讨论: Debate 参数: content=思考/理由 \n",
    "# - 玩家信息: GetAllPlayersName 参数: 无 \n",
    "# - 死亡遗言: DeathWords 参数: content=给予玩家线索\n",
    "# - 玩家弃权: Pass 参数: 无 \n",
    "# - 其他动作: Pass 参数: 无 \n",
    "\n",
    "@tool\n",
    "def WolfVote(target: str) -> str:\n",
    "    \"\"\"夜晚投票(狼人专属行动)\"\"\"\n",
    "    return \"WolfVote\"\n",
    "\n",
    "@tool\n",
    "def ProphetCheck(target: str) -> str:\n",
    "    \"\"\"夜晚查验(预言家专属行动)\"\"\"\n",
    "    return \"ProphetCheck\"\n",
    "\n",
    "@tool\n",
    "def PlayerDoubt(target: str) -> str:\n",
    "    \"\"\"白天怀疑(所有玩家白天可选行动, 非投票)\"\"\"\n",
    "    return \"PlayerDoubt\"\n",
    "\n",
    "@tool\n",
    "def PlayerVote(target: str) -> str:\n",
    "    \"\"\"白天投票\"\"\"\n",
    "    return \"PlayerVote\"\n",
    "\n",
    "@tool\n",
    "def Debate(target: str) -> str:\n",
    "    \"\"\"白天讨论\"\"\"\n",
    "    return \"Debate\"\n",
    "\n",
    "@tool\n",
    "def GetAllPlayersName(target: str) -> str:\n",
    "    \"\"\"玩家信息\"\"\"\n",
    "    return \"GetAllPlayersName\"\n",
    "\n",
    "@tool\n",
    "def DeathWords(content: str) -> str:\n",
    "    \"\"\"死亡遗言\"\"\"\n",
    "    return \"DeathWords\"\n",
    "\n",
    "@tool\n",
    "def Pass(content: str) -> str:\n",
    "    \"\"\"玩家弃权,说明理由\"\"\"\n",
    "    return \"Pass\"\n",
    "\n",
    " \n",
    "# print(f\"search.name:{search.name}\")\n",
    "# print(f\"search.description:{search.description}\")\n",
    "# print(f\"search.args:{search.args}\")\n",
    "\n",
    "# Define a list of tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"WolfVote\",\n",
    "        func=WolfVote.run,\n",
    "        description=\"夜晚投票(狼人专属行动)\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name = \"ProphetCheck\",\n",
    "        func=ProphetCheck.run,\n",
    "        description=\"夜晚查验(预言家专属行动)\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name = \"PlayerDoubt\",\n",
    "        func=PlayerDoubt.run,\n",
    "        description=\"白天怀疑(所有玩家白天可选行动, 非投票)\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name = \"PlayerVote\",\n",
    "        func=PlayerVote.run,\n",
    "        description=\"白天投票\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name = \"Pass\",\n",
    "        func=Pass.run,\n",
    "        description=\"玩家弃权,说明理由\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Using tools, the LLM chain and output_parser to make an agent\n",
    "tool_names = [tool.name for tool in tools]\n",
    "\n",
    "prompt = CustomPromptTemplate(\n",
    "    template=template_werewolf_role.replace(\"{tool_names}\", \",\".join(tool_names)),\n",
    "    tools=tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\", \"history\", 'agent_scratchpad', 'tools', 'tool_names']\n",
    ")\n",
    "print(prompt)\n",
    "# output_parser = CustomOutputParser()\n",
    "\n",
    "llm = claude_instant_llm\n",
    "\n",
    "# LLM chain consisting of the LLM and a prompt\n",
    "# llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "agent = create_structured_chat_agent(llm, tools, prompt)\n",
    "\n",
    "# agent = LLMSingleActionAgent(\n",
    "#     llm_chain=llm_chain, \n",
    "#     output_parser=output_parser,\n",
    "#     # We use \"Observation\" as our stop sequence so it will stop when it receives Tool output\n",
    "#     # If you change your prompt template you'll need to adjust this as well\n",
    "#     stop=[\"\\nObservation:\"], \n",
    "#     allowed_tools=tool_names\n",
    "# )\n",
    "\n",
    "# Initiate the agent that will respond to our queries\n",
    "# Set verbose=True to share the CoT reasoning the LLM goes through\n",
    "memory = ConversationBufferWindowMemory(k=2)\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, memory=memory, verbose=False)\n",
    "\n",
    "agent_executor.invoke({\"input\":\"第一个晚上，你支持的玩家该如何行动？\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a82dcee-a14c-4bee-927a-ea230129b227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.GamePlayer import GamePlayer\n",
    "from utils.GameMaster import GameMaster\n",
    "from utils.PeTemplates import *\n",
    "from utils import ParseJson, print_ww, Print, Info, Debug, Warn, Error\n",
    "\n",
    "llm = claude_instant_llm\n",
    "GM = GameMaster(10, llm, False)\n",
    "GM.ResetGame()\n",
    "GM.RunGame()\n",
    "GM.EndGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfcc21a-1ecf-4f3b-b73e-8d74baadb46e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
